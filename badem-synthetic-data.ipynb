{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192da6d5",
   "metadata": {},
   "source": [
    "# Badem: Context-Aware Spell Checking Seq2Seq Model for Turkish\n",
    "\n",
    "The notebook consists of two parts:\n",
    "\n",
    "- First part is where we use [TS Wikipedia Corpus](https://tscorpus.com/corpora/ts-wikipedia-corpus/), that consists of 45,245,304 million tokens, to generate the dataset to train the model.\n",
    "- Second part is the actual training.\n",
    "\n",
    "## Synthetic Error Generation\n",
    "\n",
    "- This part is aimed to generate synthetic errors with 1-2-3 Edit Distances from the original words with error characters being selected based on the neighborhood of original characters in Turkish Q keyboard. \n",
    "\n",
    "- Generation consists of 3 different operations: replacement, insertion, deletion.\n",
    "\n",
    "### If generated dataset is provided, you can skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4019becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27a43c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lowercase_tr (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_2_sentence_tokenizer = (x)  -> x # split(x, ('.'))\n",
    "\n",
    "tokenizer = split\n",
    "\n",
    "lower_chars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'ğ', 'ı', 'ü', 'ö', 'ç', 'ş', 'w', 'x', 'q', '.', ',', ' ']\n",
    "upper_chars = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'İ', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z', 'Ğ', 'I', 'Ü', 'Ö', 'Ç', 'Ş', 'W', 'X', 'Q', '.', ',', ' '];\n",
    "\n",
    "\n",
    "function lowercase_tr(str)\n",
    "    new_word = []\n",
    "    \n",
    "    for c in str\n",
    "        locs = findall(x-> c==x, upper_chars)\n",
    "        if length(locs) != 0\n",
    "            push!(new_word, lower_chars[locs[1]])\n",
    "        else\n",
    "            push!(new_word, c)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    join(new_word)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f032e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ts_wikipedia-export.txt\"\n",
    "\n",
    "io = open(filename);\n",
    "lines = readlines(io)\n",
    "close(io)\n",
    "\n",
    "sentences = lowercase_tr.(lines)\n",
    "seperated_sentences = tokenizer.(sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "584eb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "\n",
    "# Neighbor lookup calculation\n",
    "# Ran only once, it is not necessary to be calculated again. This code snippet below is to calculate \n",
    "# the distance between each letter.\n",
    "\n",
    "key_positions = Dict(\n",
    "        'q' => (1,1),\n",
    "        'w' => (2,1),\n",
    "        'e' => (3,1),\n",
    "        'r' => (4,1),\n",
    "        't' => (5,1),\n",
    "        'y' => (6,1),\n",
    "        'u' => (7,1),\n",
    "        'ı' => (8,1),\n",
    "        'o' => (9,1),\n",
    "        'p' => (10,1),\n",
    "        'ğ' => (11,1),\n",
    "        'ü' => (12,1),\n",
    "        'a' => (1.25,2),\n",
    "        's' => (2.25,2),\n",
    "        'd' => (3.25,2),\n",
    "        'f' => (4.25,2),\n",
    "        'g' => (5.25,2),\n",
    "        'h' => (6.25,2),\n",
    "        'j' => (7.25,2),\n",
    "        'k' => (8.25,2),\n",
    "        'l' => (9.25,2),\n",
    "        'ş' => (10.25,2),\n",
    "        'i' => (11.25,2),\n",
    "        'z' => (1.75,3),\n",
    "        'x' => (2.75,3),\n",
    "        'c' => (3.75,3),\n",
    "        'v' => (4.75,3),\n",
    "        'b' => (5.75,3),\n",
    "        'n' => (6.75,3),\n",
    "        'm' => (7.75,3),\n",
    "        'ö' => (8.75,3),\n",
    "        'ç' => (9.75,3)  \n",
    "); \n",
    "\n",
    "neighbor_lookup = Dict()\n",
    "\n",
    "for (c, pos) in key_positions\n",
    "    neighbors = collect(key_positions)\n",
    "\n",
    "    sort!(neighbors, lt= (x,y) -> sum((x[2].-pos).^2) < sum((y[2].-pos).^2))\n",
    "    neighbor_lookup[c] = map(x->x[1], neighbors[2:6])\n",
    "end\n",
    "=#\n",
    "\n",
    "# 5 closest neighbors of each key\n",
    "neighbor_lookup = Dict('n' => ['m', 'b', 'h', 'j', 'k'], 'f' => ['d', 'g', 'r', 'c', 'v'], 'w' => ['e', 'q', 's', 'a', 'd'], 'ç' => ['ö', 'ş', 'l', 'k', 'i'], 'd' => ['f', 's', 'e', 'c', 'x'], 'e' => ['w', 'r', 'd', 's', 'f'], 'o' => ['ı', 'p', 'l', 'k', 'ş'], 'ı' => ['o', 'u', 'k', 'j', 'l'], 'h' => ['j', 'g', 'y', 'n', 'b'], 'y' => ['t', 'u', 'h', 'g', 'j'], 's' => ['d', 'a', 'w', 'z', 'x'], 'r' => ['e', 't', 'f', 'd', 'g'], 't' => ['y', 'r', 'g', 'f', 'h'], 'j' => ['h', 'k', 'u', 'n', 'm'], 'k' => ['j', 'l', 'ı', 'ö', 'm'], 'q' => ['w', 'a', 's', 'e', 'z'], 'ğ' => ['p', 'ü', 'i', 'ş', 'o'], 'ş' => ['i', 'l', 'p', 'ç', 'ğ'], 'i' => ['ş', 'ğ', 'ü', 'p', 'ç'], 'ö' => ['ç', 'm', 'k', 'l', 'j'], 'a' => ['s', 'q', 'z', 'w', 'x'], 'c' => ['x', 'v', 'f', 'd', 's'], 'p' => ['o', 'ğ', 'ş', 'l', 'i'], 'ü' => ['ğ', 'i', 'p', 'ş', 'l'], 'm' => ['n', 'ö', 'j', 'k', 'h'], 'z' => ['x', 's', 'a', 'd', 'c'], 'g' => ['f', 'h', 't', 'v', 'b'], 'x' => ['c', 'z', 'd', 's', 'f'], 'u' => ['ı', 'y', 'j', 'h', 'k'], 'l' => ['k', 'ş', 'o', 'ç', 'ö'], 'v' => ['c', 'b', 'f', 'g', 'd'], 'b' => ['n', 'v', 'h', 'g', 'f']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c14e69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_englishification (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function apply_replacement(word, dist=2)\n",
    "    if dist > length(word)\n",
    "        return word\n",
    "    end\n",
    "    \n",
    "    indices = sample(1:length(word), dist, replace = false)   \n",
    "    chars =   rand(1:5, dist) \n",
    "\n",
    "    new_word = []\n",
    "    j = 1\n",
    "        \n",
    "    for (i, c) in enumerate(word)\n",
    "        if i in indices\n",
    "            new_char_lookup = get(neighbor_lookup, c, c)\n",
    "            if length(new_char_lookup) == 5\n",
    "                push!(new_word, new_char_lookup[chars[j]])\n",
    "            else\n",
    "                push!(new_word, c)\n",
    "            end\n",
    "            \n",
    "            j = j + 1\n",
    "        else\n",
    "            push!(new_word, c)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return join(new_word)\n",
    "end\n",
    "\n",
    "function apply_insert(word, dist=2)\n",
    "    \n",
    "    indices = sample(1:(length(word)+dist), dist, replace = false)   \n",
    "    chars =   rand(1:5, dist) \n",
    "\n",
    "    new_word = []\n",
    "    j = 1\n",
    "    \n",
    "    for (i, c) in enumerate(word)\n",
    "        if i in indices\n",
    "            new_char_lookup = get(neighbor_lookup, c, c)\n",
    "            if length(new_char_lookup) == 5\n",
    "                push!(new_word, new_char_lookup[chars[j]])\n",
    "            end\n",
    "            push!(new_word, c)\n",
    "            j = j + 1\n",
    "            \n",
    "        else\n",
    "            push!(new_word, c)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return join(new_word)\n",
    "end\n",
    "\n",
    "function apply_deletion(word, dist=2)\n",
    "    if dist > length(word)\n",
    "        return word\n",
    "    end\n",
    "    \n",
    "    indices = sample(1:length(word), dist, replace = false)   \n",
    "    \n",
    "    new_word = []\n",
    "    \n",
    "    for (i, c) in enumerate(word)\n",
    "        if !(i in indices)\n",
    "            push!(new_word, c)            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return join(new_word)\n",
    "end\n",
    "\n",
    "\n",
    "#Currently not in use\n",
    "function apply_englishification(word)  \n",
    "    return replace(word, \"ı\"=>\"i\", \"ç\"=>\"c\", \"ş\"=>\"s\", \"ğ\"=>\"g\", \"ü\"=>\"u\", \"ö\"=>\"o\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb7a016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edit_methods = [apply_deletion, apply_insert, apply_replacement]\n",
    "\n",
    "errored_sentences = []\n",
    "correct_words = []\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "for sentence in seperated_sentences[:]\n",
    "    if length(sentence) < window_size * 2 + 1\n",
    "        continue\n",
    "    end\n",
    "    \n",
    "    \n",
    "    for j in 1: Int(floor((length(sentence))/(window_size*2+1)))\n",
    "        i = (j-1) * (2*window_size + 1) + window_size + 1\n",
    "        \n",
    "        if length(sentence[i]) < 3\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        push!(correct_words, sentence[i])\n",
    "        sentence[i] = \"∑\" * edit_methods[rand(1:3)](sentence[i], rand(1:3)) * \"∑\"\n",
    "\n",
    "        push!(errored_sentences, \"Ω\"*join(sentence[i-window_size:i+window_size], \" \")*\"Ω\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8b8d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = Int(trunc(length(errored_sentences) * 4/5))\n",
    "\n",
    "outfile = \"shorter-train-sentences-\" * filename\n",
    "f = open(outfile, \"w\")\n",
    "\n",
    "for i in errored_sentences[1:split_num] # or for note in notes\n",
    "    println(f, i)\n",
    "end\n",
    "\n",
    "close(f);\n",
    "\n",
    "outfile = \"shorter-train-words-\" * filename\n",
    "f = open(outfile, \"w\")\n",
    "\n",
    "for i in correct_words[1:split_num] # or for note in notes\n",
    "    println(f, i)\n",
    "end\n",
    "\n",
    "close(f);\n",
    "\n",
    "\n",
    "outfile = \"shorter-test-sentences-\" * filename\n",
    "f = open(outfile, \"w\")\n",
    "\n",
    "for i in errored_sentences[split_num+1:end] # or for note in notes\n",
    "    println(f, i)\n",
    "end\n",
    "\n",
    "close(f);\n",
    "\n",
    "outfile = \"shorter-test-words-\" * filename\n",
    "f = open(outfile, \"w\")\n",
    "\n",
    "for i in correct_words[split_num+1:end] # or for note in notes\n",
    "    println(f, i)\n",
    "end\n",
    "\n",
    "close(f);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa680c8",
   "metadata": {},
   "source": [
    "## S2S Model For Spell Checking\n",
    "\n",
    "The model uses characters as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3f7893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_chars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'ğ', 'ı', 'ü', 'ö', 'ç', 'ş', 'w', 'x', 'q', '.', ',', ' ']\n",
    "upper_chars = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'İ', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z', 'Ğ', 'I', 'Ü', 'Ö', 'Ç', 'Ş', 'W', 'X', 'Q', '.', ',', ' '];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19f04c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Test, Base.Iterators, IterTools, Random # , LinearAlgebra, StatsBase\n",
    "using AutoGrad: @gcheck  # to check gradients, use with Float64\n",
    "Knet.atype() = KnetArray{Float32}  # determines what Knet.param() uses.\n",
    "#macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26ff1374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "char_to_token (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_token_order = ['Ω', '∑', 'π', ';', '!', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'ğ', 'ı', 'ü', 'ö', 'ç', 'ş', 'w', 'x', 'q', '.', ',', ' ']\n",
    "\n",
    "EMBED_SIZE = length(char_to_token_order)\n",
    "\n",
    "function char_to_token(str)\n",
    "    tokens = []\n",
    "    for char in str\n",
    "       res = findfirst(x->x==char, char_to_token_order)\n",
    "\n",
    "       if res != nothing\n",
    "            push!(tokens, res)\n",
    "       else\n",
    "            push!(tokens, 3)\n",
    "       end\n",
    "    end\n",
    "    \n",
    "    return tokens\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f855389",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TextReader\n",
    "    filename::String\n",
    "end\n",
    "\n",
    "window = 2\n",
    "EOS = 1\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    if s == nothing\n",
    "        f = open(r.filename, \"r\")\n",
    "        s = (f, readline(f));\n",
    "    elseif eof(s[1])\n",
    "        close(s[1]);\n",
    "        return nothing;\n",
    "    end\n",
    "    (f, line) = s;\n",
    "    \n",
    "    seperated_line = split(line)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokens_in_line = char_to_token.(line)\n",
    "    \n",
    "    return tokens_in_line, (f, readline(f))     \n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d3a4718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask! (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    # Your code here\n",
    "    Embed(param(embedsize, vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    # Your code here\n",
    "    l.w[:, x]\n",
    "end\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    # Your code here\n",
    "    Linear(param(outputsize, inputsize), param0(outputsize, 1))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    # Your code here\n",
    "    l.w * x .+ l.b\n",
    "end\n",
    "\n",
    "function mask!(a,pad)\n",
    "    # Your code here\n",
    "    for i in 1:size(a)[1]\n",
    "       for j in Iterators.reverse(2:size(a)[2])\n",
    "            if a[i,j-1] == pad\n",
    "                a[i,j] = 0\n",
    "            else\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    " \n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fcb74ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arraybatch (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS = 1\n",
    "\n",
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(src::TextReader,  tgt::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 10, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar source sentence length\n",
    "    MTData(src, tgt, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::MTData, state=nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets; empty!(b); end\n",
    "    end\n",
    "    bucket,ibucket = nothing,nothing\n",
    "    while true\n",
    "        iter = (state === nothing ? iterate(Iterators.zip(d.src, d.tgt)) : iterate(Iterators.zip(d.src, d.tgt), state))\n",
    "        if iter === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent, state = iter\n",
    "            if length(sent[1]) > d.maxlength || length(sent[1]) == 0; continue; end\n",
    "            \n",
    "            ibucket = min(1 + (length(sent[1])-1) ÷ d.bucketwidth, length(d.buckets))\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, sent)\n",
    "            \n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    batchsize = length(bucket)\n",
    "    batch = arraybatch(d, bucket)\n",
    "    empty!(bucket)\n",
    "    \n",
    "    return batch, state #a, state\n",
    "end\n",
    "\n",
    "function arraybatch(d::MTData, bucket)\n",
    "    # Your code here\n",
    "    src_max = maximum(x -> length(x[1]), bucket);\n",
    "    tgt_max = maximum(x -> length(x[2]), bucket);\n",
    "    \n",
    "    batch_x = fill(EOS, d.batchsize, src_max)\n",
    "    batch_y = fill(EOS, d.batchsize, tgt_max+2)\n",
    "    \n",
    "    for (i, (src_senten, tgt_senten)) in enumerate(bucket)\n",
    "        for j in 1:length(src_senten)\n",
    "            batch_x[i, j+(src_max-length(src_senten))] = src_senten[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for (i, (src_senten, tgt_senten)) in enumerate(bucket)\n",
    "        for j in 1:length(tgt_senten)\n",
    "            batch_y[i, j+1] = tgt_senten[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (batch_x, batch_y)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef0a107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S_v1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS = 1\n",
    "\n",
    "struct S2S_v1\n",
    "    srcembed::Embed     # source language embedding\n",
    "    encoder::RNN        # encoder RNN (can be bidirectional)\n",
    "    tgtembed::Embed     # target language embedding\n",
    "    decoder::RNN        # decoder RNN\n",
    "    projection::Linear  # converts decoder output to vocab scores\n",
    "    dropout::Real       # dropout probability to prevent overfitting\n",
    "end\n",
    "\n",
    "function S2S_v1(hidden::Int,         # hidden size for both the encoder and decoder RNN\n",
    "                srcembsz::Int,       # embedding size for source language\n",
    "                tgtembsz::Int;      # embedding size for target language\n",
    "                layers=1,            # number of layers\n",
    "                bidirectional=false, # whether encoder RNN is bidirectional\n",
    "                dropout=0)           # dropout probability\n",
    "    # Your code here\n",
    "    S2S_v1(Embed(EMBED_SIZE, srcembsz), RNN(srcembsz, hidden; bidirectional, dropout, numLayers=layers, atype= Knet.atype()), Embed(EMBED_SIZE, tgtembsz), RNN(tgtembsz, hidden; dropout, numLayers=(layers*(bidirectional ? 2 : 1)), atype=Knet.atype()), Linear(hidden, EMBED_SIZE), dropout)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bf02cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc RNN\n",
    "\n",
    "function (s::S2S_v1)(src, tgt; average=true)\n",
    "    # Your code here\n",
    "    s.encoder.h = 0\n",
    "    s.encoder(s.srcembed(src))\n",
    "\n",
    "    s.decoder.h = s.encoder.h\n",
    "\n",
    "    z = s.decoder(s.tgtembed(tgt[:, 1:end-1]))\n",
    "    \n",
    "    pred = s.projection(reshape(z, :, size(z)[2]*size(z)[3]))\n",
    "    ans  = reshape(mask!(tgt[:,2:end], EOS), size(tgt[:, 2:end])[1]*size(tgt[:,2:end])[2])\n",
    "        \n",
    "    return nll(pred, ans; average)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b0997c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing S2S_v1\n",
      "└ @ Main In[33]:1\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] (::S2S_v1)(src::Matrix{Int64}, tgt::Matrix{Int64}; average::Bool)",
      "   @ Main ./In[32]:15",
      " [2] top-level scope",
      "   @ In[33]:16",
      " [3] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "@info \"Testing S2S_v1\"\n",
    "Random.seed!(1)\n",
    "model = S2S_v1(512, 512, 512, layers=2, bidirectional=true, dropout=0.2)\n",
    "\n",
    "train_sentences =  TextReader(\"shorter-train-sentences-ts_wikipedia-export.txt\")\n",
    "test_sentences  =  TextReader(\"shorter-test-sentences-ts_wikipedia-export.txt\")\n",
    "\n",
    "train_words = TextReader(\"shorter-train-words-ts_wikipedia-export.txt\")\n",
    "test_words  = TextReader(\"shorter-test-words-ts_wikipedia-export.txt\")\n",
    "\n",
    "dtrn = MTData(train_sentences, train_words)\n",
    "ddev = MTData(test_sentences,  test_words)\n",
    "\n",
    "(x, y) = first(dtrn)\n",
    "\n",
    "model(x,y; average=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b31241b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, data; average=true)\n",
    "    # Your code here\n",
    "    x, y = first(data)\n",
    "    🥲 = model(x, y; average = average) \n",
    "    \n",
    "    for (x, y) in drop(data, 1)\n",
    "        🥲 = model(x, y; average = average) .+ 🥲\n",
    "    end\n",
    "    \n",
    "    return 🥲 ./ (average ? length(data) : 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ddb7fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7120-element Vector{Tuple{T, T} where T}:\n",
       " ([1 1 … 23 1; 1 1 … 38 1; … ; 1 1 … 3 1; 1 1 … 6 1], [1 10 … 1 1; 1 23 … 1 1; … ; 1 12 … 1 1; 1 18 … 1 1])\n",
       " ([1 1 … 19 1; 1 1 … 6 1; … ; 1 1 … 10 1; 1 1 … 19 1], [1 18 … 1 1; 1 23 … 1 1; … ; 1 21 … 1 1; 1 7 … 1 1])\n",
       " ([1 1 … 18 1; 1 1 … 6 1; … ; 1 1 … 38 1; 1 1 … 10 1], [1 10 … 1 1; 1 6 … 1 1; … ; 1 7 … 1 1; 1 23 … 1 1])\n",
       " ([1 1 … 22 1; 1 1 … 22 1; … ; 1 1 … 38 1; 1 1 … 31 1], [1 23 … 1 1; 1 27 … 1 1; … ; 1 14 … 1 1; 1 20 … 1 1])\n",
       " ([1 1 … 19 1; 1 1 … 6 1; … ; 1 24 … 22 1; 1 1 … 14 1], [1 23 … 1 1; 1 7 … 1 1; … ; 1 16 … 1 1; 1 13 … 1 1])\n",
       " ([1 1 … 30 1; 1 1 … 19 1; … ; 1 1 … 19 1; 1 1 … 23 1], [1 12 … 1 1; 1 27 … 1 1; … ; 1 25 … 1 1; 1 6 … 1 1])\n",
       " ([1 1 … 30 1; 1 1 … 18 1; … ; 1 1 … 6 1; 1 1 … 22 1], [1 14 … 1 1; 1 20 … 1 1; … ; 1 14 … 1 1; 1 18 … 1 1])\n",
       " ([1 1 … 31 1; 1 39 … 19 1; … ; 1 1 … 14 1; 1 27 … 14 1], [1 16 … 1 1; 1 19 … 1 1; … ; 1 16 … 1 1; 1 16 … 1 1])\n",
       " ([1 1 … 39 1; 1 1 … 19 1; … ; 1 1 … 19 1; 1 33 … 6 1], [1 3 … 1 1; 1 3 … 1 1; … ; 1 13 … 1 1; 1 24 … 1 1])\n",
       " ([1 1 … 23 1; 1 1 … 30 1; … ; 1 1 … 19 1; 1 20 … 19 1], [1 24 … 1 1; 1 23 … 1 1; … ; 1 31 … 1 1; 1 18 … 1 1])\n",
       " ([1 1 … 38 1; 1 1 … 19 1; … ; 1 1 … 17 1; 1 1 … 10 1], [1 24 … 1 1; 1 12 … 1 1; … ; 1 18 … 1 1; 1 23 … 1 1])\n",
       " ([1 1 … 23 1; 1 1 … 22 1; … ; 1 26 … 19 1; 1 1 … 31 1], [1 23 … 1 1; 1 16 … 1 1; … ; 1 20 … 1 1; 1 12 … 1 1])\n",
       " ([1 1 … 24 1; 1 1 … 19 1; … ; 1 1 … 25 1; 1 1 … 6 1], [1 22 … 1 1; 1 12 … 1 1; … ; 1 23 … 1 1; 1 6 … 1 1])\n",
       " ⋮\n",
       " ([1 1 … 10 1; 1 1 … 23 1; … ; 1 1 … 6 1; 1 31 … 22 1], [1 12 … 1 1; 1 27 … 1 1; … ; 1 3 … 1 1; 1 7 … 1 1])\n",
       " ([1 38 … 10 1; 1 1 … 16 1; … ; 1 1 … 18 1; 1 1 … 6 1], [1 23 … 1 1; 1 15 … 1 1; … ; 1 7 … 1 1; 1 6 … 1 1])\n",
       " ([1 1 … 3 1; 1 1 … 39 1; … ; 1 27 … 14 1; 1 1 … 39 1], [1 16 … 1 1; 1 22 … 1 1; … ; 1 9 … 1 1; 1 24 … 1 1])\n",
       " ([1 1 … 23 1; 1 1 … 6 1; … ; 1 1 … 10 1; 1 1 … 22 1], [1 6 … 1 1; 1 21 … 1 1; … ; 1 20 … 1 1; 1 16 … 1 1])\n",
       " ([1 38 … 39 1; 1 1 … 10 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 22 … 1 1; 1 3 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])\n",
       " ([1 19 … 3 1; 1 1 … 16 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 33 … 1 1; 1 12 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])\n",
       " ([1 18 … 19 1; 1 1 … 18 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 16 … 1 1; 1 24 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])\n",
       " ([1 1 … 22 1; 1 1 … 18 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 16 … 1 1; 1 18 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])\n",
       " ([1 1 … 22 1; 1 1 … 10 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 33 … 1 1; 1 17 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])\n",
       " ([1 1 … 24 1; 1 1 … 6 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 23 … 1 1; 1 34 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])\n",
       " ([1 1 … 10 1; 1 1 … 19 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 24 … 1 1; 1 7 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])\n",
       " ([1 1 … 10 1; 1 1 … 6 1; … ; 1 1 … 1 1; 1 1 … 1 1], [1 20 … 1 1; 1 31 … 1 1; … ; 1 1 … 1 1; 1 1 … 1 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@info \"Testing loss\"\n",
    "#@test loss(model, dtst, average=false) == (1.0427646f6, 105937)\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "# Also, because we do not mask src, different batch sizes may lead to slightly different\n",
    "# losses. The test above gives (1.0430301f6, 105937) with batchsize==1.\n",
    "\n",
    "function train!(model, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn), steps=100) do y\n",
    "        losses = [ loss(model, d) for d in (dev,tst...) ]\n",
    "        if losses[1] < bestloss\n",
    "            bestmodel, bestloss = deepcopy(model), losses[1]\n",
    "        end\n",
    "        return (losses...,)\n",
    "    end\n",
    "    return bestmodel\n",
    "end\n",
    "\n",
    "epochs = 10\n",
    "ctrn   = collect(dtrn)\n",
    "trnx10 = collect(flatten(shuffle!(first(ctrn, 2500)) for i in 1:epochs))\n",
    "trn20  = ctrn[1:20]\n",
    "dev38  = collect(ddev)\n",
    "# Uncomment this to train the model (This takes about 30 mins on a V100, 60 mins on a T4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f22a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 04:40:10/04:40:10, 1.49i/s] (1.4826353f0, 1.2705247f0)\n"
     ]
    }
   ],
   "source": [
    "model  = train!(model, trnx10, dev38, trn20)\n",
    "# Uncomment this to save the model:\n",
    "Knet.save(\"shorter-s2s_v1.jld2\",\"model\", model)\n",
    "# Uncomment this to load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf9493cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src::Matrix{Int}; stopfactor = 3)\n",
    "    # Your code here\n",
    "    s.encoder.h = 0\n",
    "    s.encoder(s.srcembed(src))\n",
    "    \n",
    "    s.decoder.h = value(s.encoder.h)\n",
    "    in_decoder = s.tgtembed(fill(EOS, size(src)[1] ))\n",
    "    \n",
    "    sentences = []\n",
    "    num_steps = 0\n",
    "    \n",
    "    ended_ones = fill(false, size(src)[1])\n",
    "    \n",
    "    while true\n",
    "        z = s.decoder(in_decoder)\n",
    "        scores = s.projection(z)\n",
    "        indices = getindex.(argmax(scores, dims=1), 1)\n",
    "        pred = reshape(indices, 1, length(indices))\n",
    "        \n",
    "        ended_ones = reshape(collect(pred .== EOS), size(src)[1] ) .| ended_ones\n",
    "        \n",
    "        pred[ended_ones] .= EOS\n",
    "        \n",
    "        push!(sentences, pred)\n",
    "        \n",
    "        num_steps += 1\n",
    "        in_decoder = z\n",
    "        \n",
    "        if num_steps >= stopfactor * size(src,2) || findfirst(x->!x, ended_ones) == nothing\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    " \n",
    "    return (char_to_token_order[transpose(reduce(vcat, sentences))])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7175362a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω'; 'b' 's' 'o' 'e' 'a' 'Ω']"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Generating some translations\n",
      "└ @ Main In[52]:14\n"
     ]
    }
   ],
   "source": [
    "# Utility to convert int arrays to sentence strings\n",
    "function int2str(y,vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w->!isequal(w,vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1+length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end\n",
    "\n",
    "# Uncomment and run these lines if you get a \"CUDNNError: CUDNN_STATUS_INTERNAL_ERROR (code 4)\" error from the cell below.\n",
    "# Knet.save(\"s2s_v1.jld2\",\"model\",model)\n",
    "model = Knet.load(\"shorter-s2s_v1.jld2\",\"model\")\n",
    "\n",
    "@info \"Generating some translations\"\n",
    "#d = MTData(tr_dev, en_dev, batchsize=1) |> collect\n",
    "\n",
    "(src,tgt) = first(ddev)\n",
    "\n",
    "out = model(src)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691e854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
